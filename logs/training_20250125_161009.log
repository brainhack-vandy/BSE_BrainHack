2025-01-25 16:10:09,180 [INFO] Starting training with config: {
  "data_dir": "output",
  "log_dir": "logs",
  "checkpoint_dir": "checkpoints",
  "sequence_length": 10,
  "batch_size": 32,
  "learning_rate": 0.0001,
  "num_epochs": 100,
  "device": "cpu",
  "model_dim": 512,
  "n_layers": 8,
  "n_heads": 8,
  "train_ratio": 0.7,
  "val_ratio": 0.15
}
2025-01-25 16:10:09,180 [INFO] Train patients: 4, Val patients: 1, Test patients: 2
2025-01-25 16:10:09,250 [INFO] NumExpr defaulting to 8 threads.
2025-01-25 16:10:11,239 [INFO] 
Epoch 1/100
2025-01-25 16:10:11,550 [INFO] Batch 0/2615, Loss: 0.948292
2025-01-25 16:10:24,616 [INFO] Batch 100/2615, Loss: 0.341272
2025-01-25 16:10:38,652 [INFO] Batch 200/2615, Loss: 0.335741
2025-01-25 16:10:52,982 [INFO] Batch 300/2615, Loss: 0.320711
2025-01-25 16:11:07,044 [INFO] Batch 400/2615, Loss: 0.312527
2025-01-25 16:11:21,305 [INFO] Batch 500/2615, Loss: 0.302191
2025-01-25 16:11:35,997 [INFO] Batch 600/2615, Loss: 0.288571
2025-01-25 16:11:49,481 [INFO] Batch 700/2615, Loss: 0.277735
2025-01-25 16:12:03,226 [INFO] Batch 800/2615, Loss: 0.268780
2025-01-25 16:12:17,256 [INFO] Batch 900/2615, Loss: 0.258262
2025-01-25 16:12:30,963 [INFO] Batch 1000/2615, Loss: 0.252073
2025-01-25 16:12:44,405 [INFO] Batch 1100/2615, Loss: 0.241282
2025-01-25 16:12:57,589 [INFO] Batch 1200/2615, Loss: 0.232303
2025-01-25 16:13:11,189 [INFO] Batch 1300/2615, Loss: 0.221985
2025-01-25 16:13:25,886 [INFO] Batch 1400/2615, Loss: 0.213632
2025-01-25 16:13:39,826 [INFO] Batch 1500/2615, Loss: 0.207245
2025-01-25 16:13:53,519 [INFO] Batch 1600/2615, Loss: 0.199157
2025-01-25 16:14:07,610 [INFO] Batch 1700/2615, Loss: 0.189129
2025-01-25 16:14:21,236 [INFO] Batch 1800/2615, Loss: 0.180916
2025-01-25 16:14:36,181 [INFO] Batch 1900/2615, Loss: 0.175892
2025-01-25 16:14:50,084 [INFO] Batch 2000/2615, Loss: 0.165588
2025-01-25 16:15:04,080 [INFO] Batch 2100/2615, Loss: 0.159263
2025-01-25 16:15:18,015 [INFO] Batch 2200/2615, Loss: 0.154021
2025-01-25 16:15:31,492 [INFO] Batch 2300/2615, Loss: 0.148433
2025-01-25 16:15:45,180 [INFO] Batch 2400/2615, Loss: 0.139444
2025-01-25 16:15:58,617 [INFO] Batch 2500/2615, Loss: 0.132474
